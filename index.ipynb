{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification using Naive Bayes - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll make use of our newfound Bayesian knowledge to classify emails as spam or not spam from the [UCI Machine Learning Repository's Spambase Dataset](https://archive.ics.uci.edu/ml/datasets/spambase).  \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Work with a real-world dataset from the UCI Machine Learning Repository\n",
    "* Classify emails as spam or not spam by making use of Naive Bayesian Classification\n",
    "* Evaluate the quality of our classifier by building a Confusion Matrix\n",
    "\n",
    "## Let's get started!\n",
    "\n",
    "Run the cell below to import everything we'll need for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# Do not change the random seed, or else the tests will fail!\n",
    "np.random.seed(0)\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we'll be working with the [Spambase Dataset](https://archive.ics.uci.edu/ml/datasets/spambase) from [UC Irvine's Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). \n",
    "\n",
    "This dataset contains emails that have already been vectorized, as well as summary statistics about each email that can also be useful in classification.  In this case, the Data Dictionary containing the names and descriptions of each column is stored in a separate file from the dataset itself.  For ease of use, we have included the `spambase.csv` file in this repo.  However, we have not included the Data Dictionary and column names.  \n",
    "\n",
    "In the cell below, read in the data from `spambase.csv`, store it in a DataFrame, and print the head.  \n",
    "\n",
    "**_HINT:_** By default, pandas will automatically assume that the first row contains metadata containing the column names. Since our dataset does not have a row of metadata, pandas will mistakenly assume the values for the first email are the column names for each column.  You can prevent this by setting the `header` parameter to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Do not change variable name!\n",
    "df = pd.read_csv('spambase.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.40</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.44</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6 ...  0.40  \\\n",
       "0  0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00   \n",
       "1  0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01   \n",
       "2  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00   \n",
       "3  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00   \n",
       "4  0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00 ...  0.00   \n",
       "\n",
       "    0.41  0.42  0.778   0.43   0.44  3.756   61   278  1  \n",
       "0  0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1  0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2  0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3  0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4  0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_freq_make',\n",
       " 'word_freq_address',\n",
       " 'word_freq_all',\n",
       " 'word_freq_3d',\n",
       " 'word_freq_our',\n",
       " 'word_freq_over',\n",
       " 'word_freq_remove',\n",
       " 'word_freq_internet',\n",
       " 'word_freq_order',\n",
       " 'word_freq_mail',\n",
       " 'word_freq_receive',\n",
       " 'word_freq_will',\n",
       " 'word_freq_people',\n",
       " 'word_freq_report',\n",
       " 'word_freq_addresses',\n",
       " 'word_freq_free',\n",
       " 'word_freq_business',\n",
       " 'word_freq_email',\n",
       " 'word_freq_you',\n",
       " 'word_freq_credit',\n",
       " 'word_freq_your',\n",
       " 'word_freq_font',\n",
       " 'word_freq_000',\n",
       " 'word_freq_money',\n",
       " 'word_freq_hp',\n",
       " 'word_freq_hpl',\n",
       " 'word_freq_george',\n",
       " 'word_freq_650',\n",
       " 'word_freq_lab',\n",
       " 'word_freq_labs',\n",
       " 'word_freq_telnet',\n",
       " 'word_freq_857',\n",
       " 'word_freq_data',\n",
       " 'word_freq_415',\n",
       " 'word_freq_85',\n",
       " 'word_freq_technology',\n",
       " 'word_freq_1999',\n",
       " 'word_freq_parts',\n",
       " 'word_freq_pm',\n",
       " 'word_freq_direct',\n",
       " 'word_freq_cs',\n",
       " 'word_freq_meeting',\n",
       " 'word_freq_original',\n",
       " 'word_freq_project',\n",
       " 'word_freq_re',\n",
       " 'word_freq_edu',\n",
       " 'word_freq_table',\n",
       " 'word_freq_conference',\n",
       " 'char_freq_;',\n",
       " 'char_freq_(',\n",
       " 'char_freq_[',\n",
       " 'char_freq_!',\n",
       " 'char_freq_$',\n",
       " 'char_freq_#',\n",
       " 'capital_run_length_average',\n",
       " 'capital_run_length_longest',\n",
       " 'capital_run_length_total',\n",
       " 'is_spam']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_list = []\n",
    "for i in range(48):\n",
    "    string_list.append('word_freq_')\n",
    "suffixes = [\n",
    "    'make',\n",
    "    'address',\n",
    "    'all',\n",
    "    '3d',\n",
    "    'our',\n",
    "    'over',\n",
    "    'remove',\n",
    "    'internet',\n",
    "    'order',\n",
    "    'mail',\n",
    "    'receive',\n",
    "    'will',\n",
    "    'people',\n",
    "    'report',\n",
    "    'addresses',\n",
    "    'free',\n",
    "    'business',\n",
    "    'email',\n",
    "    'you',\n",
    "    'credit',\n",
    "    'your',\n",
    "    'font',\n",
    "    '000',\n",
    "    'money',\n",
    "    'hp',\n",
    "    'hpl',\n",
    "    'george',\n",
    "    '650',\n",
    "    'lab',\n",
    "    'labs',\n",
    "    'telnet',\n",
    "    '857',\n",
    "    'data',\n",
    "    '415',\n",
    "    '85',\n",
    "    'technology',\n",
    "    '1999',\n",
    "    'parts',\n",
    "    'pm',\n",
    "    'direct',\n",
    "    'cs',\n",
    "    'meeting',\n",
    "    'original',\n",
    "    'project',\n",
    "    're',\n",
    "    'edu',\n",
    "    'table',\n",
    "    'conference',\n",
    "\n",
    "]\n",
    "\n",
    "len(suffixes)\n",
    "\n",
    "for counter in range(0,len(suffixes)):\n",
    "    string_list[counter] += suffixes[counter]\n",
    "    counter +=1\n",
    "\n",
    "string_list.append('char_freq_;')\n",
    "string_list.append('char_freq_(')\n",
    "string_list.append('char_freq_[')\n",
    "string_list.append('char_freq_!')\n",
    "string_list.append('char_freq_$')\n",
    "string_list.append('char_freq_#')\n",
    "string_list.append('capital_run_length_average')\n",
    "string_list.append('capital_run_length_longest')\n",
    "string_list.append('capital_run_length_total')\n",
    "string_list.append('is_spam')\n",
    "\n",
    "string_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataset does not contain column names.  You will need to manually grab these from the [Dataset Description](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names) and create an array containing the correct column names that we can set.  \n",
    "\n",
    "Take a minute to visit the link above and get the names of each column.  There's no python magic needed here--you'll just need to copy and paste them over in the correct order as strings in a python array.  (It's not glamorous, but it's realistic.  This is a pretty common part of the Data Science Process.)\n",
    "\n",
    "In the cell below, create the array of column names and then use this array to set the correct column names for the `df` object.  \n",
    "\n",
    "**_NOTE:_** Be sure to read the Dataset Description/Documentation carefully.  Note that the last column of the dataset (we can call it `is_spam` is the last column of the actual dataset, although the data description has it at the top, not the bottom, of the list.  Make sure you get this column name in the right place, as it will be our target variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Do not chang variable name!\n",
    "column_names = string_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Exploring the Dataset\n",
    "\n",
    "Now, in the cell below, use what you've learned to clean and explore the dataset.  Make sure you check for null values, and examine the descriptive statistics for the dataset.  \n",
    "\n",
    "Try to create at least 1 visualization during this Exploratory Data Analysis (EDA) process. \n",
    "\n",
    "Use the cells below for this step. \n",
    "\n",
    "**_Remember_**, if you need to add more cells, you can always highlight a cell, press `esc` to enter command mode, and then press `a` to add a cell above the highlighted cell, or `b` to add a cell below the highlighted cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().unique() # no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4600.000000\n",
       "mean        0.097250\n",
       "std         0.555966\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.000000\n",
       "max        18.180000\n",
       "Name: word_freq_data, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_freq_data'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['word_freq_data'], bins = 100)\n",
    "plt.autoscale(enable=True, axis = 'both', tight = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Exploration\n",
    "\n",
    "Did you notice anything interesting during your EDA? Briefly explain your approach and your findings below this line:\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "Outlier for 'word_freq_data'\n",
    "\n",
    "\n",
    "## Creating Training and Testing Sets\n",
    "\n",
    "Since we are using Naive Bayes for classification, we'll need to treat this like any other machine learning problem and create separate **_training sets_** and **_testing sets_** for **_holdout validation_**.  Otherwise, if we just trust the classifier's performance on the training set, we won't know for sure if the classifier has learned to detect spam emails in the real world, or just from this particular dataset.  \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* Store the target column in a separate variable and then remove it from the dataset. \n",
    "* Create training and testing sets using the [appropriate method](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from `sklearn`.  \n",
    "\n",
    "**_HINT:_** We want to make sure that the training and testing samples get same distribution of spam/not spam emails.  Otherwise, our model may get a training set that doesn't contain enough of one class to learn how to tell it apart from the other.  In order to deal with this problem, we can pass in the variable containing our labels to the `stratify` parameter.  For more information, see the documentation in the link above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.21               0.28           0.50           0.0   \n",
       "1            0.06               0.00           0.71           0.0   \n",
       "2            0.00               0.00           0.00           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.14            0.28              0.21                0.07   \n",
       "1           1.23            0.19              0.19                0.12   \n",
       "2           0.63            0.00              0.31                0.63   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           1.85            0.00              0.00                1.85   \n",
       "\n",
       "   word_freq_order  word_freq_mail   ...     char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.94   ...            0.00        0.132   \n",
       "1             0.64            0.25   ...            0.01        0.143   \n",
       "2             0.31            0.63   ...            0.00        0.137   \n",
       "3             0.31            0.63   ...            0.00        0.135   \n",
       "4             0.00            0.00   ...            0.00        0.223   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.372        0.180        0.048   \n",
       "1          0.0        0.276        0.184        0.010   \n",
       "2          0.0        0.137        0.000        0.000   \n",
       "3          0.0        0.135        0.000        0.000   \n",
       "4          0.0        0.000        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       5.114                         101   \n",
       "1                       9.821                         485   \n",
       "2                       3.537                          40   \n",
       "3                       3.537                          40   \n",
       "4                       3.000                          15   \n",
       "\n",
       "   capital_run_length_total  is_spam  \n",
       "0                      1028        1  \n",
       "1                      2259        1  \n",
       "2                       191        1  \n",
       "3                       191        1  \n",
       "4                        54        1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Do not change variable names!\n",
    "target = df['is_spam']\n",
    "not_target = list(df.columns)\n",
    "not_target.pop()\n",
    "not_target\n",
    "clean_df = df[not_target]\n",
    "\n",
    "\n",
    "# Test 4: Do not change variable names!\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_df, target, stratify = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting our Classifier\n",
    "\n",
    "Now that we have split our data into appropriate sets, we need to fit our classifier before we can make predictions and check our model's performance.\n",
    "\n",
    "Recall what you learned about the 3 different types of Naive Bayesian Classifiers provided by `sklearn`.  Given the distribution of our data, explain why each of the following classifier types is or isn't appropriate for this problem.\n",
    "\n",
    "**_GaussianNB:_**  \n",
    "Class values are continuous\n",
    "\n",
    "**_BernoulliNB:_** \n",
    "Class values are continuous, BNB class values are discreet\n",
    "\n",
    "**_MultinomialNB:_** \n",
    "Class values are continuous, MNB class values are discreet\n",
    "\n",
    "In the cell below, create the appropriate classifier type and then `fit()` it to the appropriate training data/labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-d76e22c112c9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-d76e22c112c9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Now that we have a fitted model, we can make predictions on our testing data.  \n",
    "\n",
    "In the cell below, use the appropriate method to make predictions on the data contained inside `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Model Performance\n",
    "\n",
    "Now that we have predictions, we can check the accuracy of our model's performance.  In order to do this, we'll use two different metrics: `accuracy_score` and `f1_score`.  For classification, accuracy is defined as the number of correct predictions (**_True Positives_** and **_True Negatives_**) divided by the total number of predictions.  \n",
    "\n",
    "**_F1 Score_** is the harmonic mean of precision and recall. This tells us the accuracy, but penalizes the classifier heavily if it favors either **_Precision_** (Spam emails correctly identified, divided by all emails predicted to be spam) or **_Recall_** (the percentage of spam emails successfully caught, out of all spam emails) too much.  Don't worry if you aren't yet familiar with these terms--we'll cover these concepts in depth in later lessons!\n",
    "\n",
    "In the cell below, use the appropriate helper functions from sklearn to get the accuracy and f1 scores for our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for model: 83.83%\n",
      "F1 Score for model: 82.62%\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Do not change variable name!\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "# Test 6: Do not change variable name!\n",
    "f1 = f1_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy Score for model: {:.4}%\".format(accuracy * 100))\n",
    "print(\"F1 Score for model: {:.4}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging Deeper: Using a Confusion Matrix\n",
    "\n",
    "Our model does pretty well, with ~81% accuracy.  However, we don't know _how_ it's failing on the 19% it got wrong.  In order to figure this out, we'll build a **_Confusion Matrix_**.\n",
    "\n",
    "For every prediction our model makes, there are four possible outcomes:\n",
    "\n",
    "**_True Positive:_** Our model predicted that the email was spam, and it was actually spam. \n",
    "\n",
    "**_True Negative:_** Our model predicted that the email was not spam, and it wasn't spam. \n",
    "\n",
    "**_False Positive:_** Our model predicted that the email was spam, but it wasn't.\n",
    "\n",
    "**_False Negative:_** Our model predicted that the email wasn't spam, but it was.  \n",
    "\n",
    "\n",
    "### Question:\n",
    "\n",
    "Which type of misclassification is preferable to the other--False Positives or False Negatives?  In this given problem, which one is preferable to the other? Explain your answer below this line:\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "False positive is worse, because it means you are missing out on seeing a message that you should see. Worst case scenario is a false negative is that you send it to spam.\n",
    "\n",
    "\n",
    "\n",
    "### Building our Confusion Matrix\n",
    "\n",
    "In the cell below, complete the `confusion_matrix` function.  This function should take in two parameters, `predictions` and `labels`, and return a dictionary counts for `'TP', 'TN', 'FP',` and `'FN'` (True Positive, True Negative, False Positive, and False Negative, respectively).  \n",
    "\n",
    "Once you have completed this function, use it to create Confusion Matrices for both the training and testing sets, and complete the tables in the following markdown cell.\n",
    "\n",
    "**_HINT:_** Your labels are currently stored in a pandas series.  To make things easier, consider converting this series to a regular old python list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Confusion Matrix: {'TP': 1297, 'TN': 1515, 'FP': 576, 'FN': 62}\n",
      "Testing Confusion Matrix: {'TP': 442, 'TN': 522, 'FP': 175, 'FN': 11}\n"
     ]
    }
   ],
   "source": [
    "# Test 7: Do not change function signature!\n",
    "def confusion_matrix(predictions, labels):\n",
    "    confusion = {}\n",
    "    labels = list(labels)\n",
    "    confusion['TP'] = 0\n",
    "    confusion['TN'] = 0\n",
    "    confusion['FP'] = 0\n",
    "    confusion['FN'] = 0\n",
    "    for index in range(len(predictions)):\n",
    "        prediction = predictions[index]\n",
    "        label = labels[index]\n",
    "        if prediction == 1 and label == 1:\n",
    "            confusion['TP'] += 1\n",
    "        elif prediction == 0 and label == 0:\n",
    "            confusion['TN'] += 1\n",
    "        elif prediction == 1 and label == 0:\n",
    "            confusion['FP'] += 1\n",
    "        else:\n",
    "            confusion['FN'] += 1\n",
    "    return confusion\n",
    "\n",
    "# Test 8: Do not change variable names!\n",
    "training_preds = clf.predict(X_train)\n",
    "\n",
    "# Test 9: Do not change variable names!\n",
    "training_cm = confusion_matrix(training_preds, y_train)\n",
    "\n",
    "# Test 10: Do not change variable names!\n",
    "testing_cm = confusion_matrix(preds, y_test)\n",
    "\n",
    "print(\"Training Confusion Matrix: {}\".format(training_cm))\n",
    "print(\"Testing Confusion Matrix: {}\".format(testing_cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intepreting Our Results\n",
    "\n",
    "Complete the tables below, and then use them to answer the following questions.\n",
    "\n",
    "\n",
    "|  Training Results  | **Is Spam** | **Is Email** |\n",
    "|:---------------:|:-------:|:--------:|:--------:\n",
    "|  **Predicted Spam** | 1297        |     576     |\n",
    "| **Predicted Email** | 64       |    1536      |\n",
    "<br>\n",
    "\n",
    "|  Testing Results  | **Is Spam** | **Is Email** |\n",
    "|:---------------:|:-------:|:--------:|\n",
    "|  **Predicted Spam** |   442      |    175      |\n",
    "| **Predicted Email** |   11      |    522      |\n",
    "\n",
    "\n",
    "How many emails are getting caught up in the spam filter? How many spam emails are getting through the filter?  Is this a model you would recommend shipping to production? Why or why not?\n",
    "________________________________________________________________________________________________________________________________\n",
    "\n",
    "Users would not get much spam in inbox but too many actual emails would end up in spam. Would not recommend - reverse issue is better if one must exist\n",
    "\n",
    "\n",
    "Don't worry about tuning the model for now--that's a lengthy process, and we'll cover it in depth in later labs.  For now, congratulations--you just built a working spam filter using Naive Bayesian Classification!\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, we:\n",
    "* Worked with a real-world dataset from the UCI Machine Learning Repository.\n",
    "* Classified emails as spam or not spam by with a Naive Bayesian Classifier. \n",
    "* Built a Confusion Matrix to evaluate the performance of our classifier.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
